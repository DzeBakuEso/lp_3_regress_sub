{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing Inventory Management: \n",
    "\n",
    "**Demand Forecasting for Enhanced Inventory Control in Favorita's Expansive Grocery Retail Chain**\n",
    "\n",
    "**Project Description:**\n",
    "\n",
    "In today's fast-paced grocery retail sector, achieving a harmonious equilibrium between readily available products and efficient inventory costs is paramount for sustainable success. The \"Optimizing Inventory Management\" initiative at Favorita seeks to innovate traditional stocking methods across its numerous outlets by leveraging the potential of machine learning for precise demand forecasting.\n",
    "\n",
    "At the heart of this project lies the ambition to rectify an operational hurdle at Favorita: ensuring consistent product availability across its diverse retail chain. The unpredictable ebb and flow of demand, influenced by seasonal shifts, changing consumer patterns, and other external factors, often results in inventory disparities. As a solution, this initiative recommends the creation and implementation of sophisticated machine learning models to accurately forecast future sales volumes.\n",
    "\n",
    "The primary goal of this endeavor is to mine rich insights from historical sales records, comprehensive product details, pricing strategies, and promotional campaigns. Through in-depth analysis of this data, using the CRISP-DM methodology, we intend to develop machine learning algorithms adept at predicting future sales across Favorita's various outlets, thereby assisting in better inventory planning and stock replenishment strategies. This rigorous analysis and predictive approach aim not only to enhance Favorita's operational efficiency but also to ensure a seamless shopping experience for its customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPOTHESIS\n",
    "\n",
    "### Null Hypothesis: Promotions do not have a positive impact on overall sales.\n",
    "\n",
    "### Alternate Hypothesis: Promotions have a positive impact on overall sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research / Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. Did the oil price affected Sales\n",
    "\n",
    "7. How did holiday events affect Sales \n",
    "\n",
    "8. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "9. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas\n",
    "%pip install -q pyodbc \n",
    "%pip install -q python-dotenv\n",
    "%pip install -q matplotlib\n",
    "%pip install -q seaborn\n",
    "%pip install -q plotly\n",
    "%pip install -q nbformat\n",
    "%pip install -q statsmodels\n",
    "%pip install -q dash\n",
    "%pip install - qwordcloud\n",
    "%pip install keras\n",
    "%pip install Cython\n",
    "%pip install tensorflow\n",
    "%pip install keras-tuner\n",
    "%pip install pmdarima\n",
    "%pip install scikeras\n",
    "%pip install joblib\n",
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for database connectivity\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Handling warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Libraries for data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go \n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical libraries and functions\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mode\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Time Series analysis libraries and functions\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Metrics and tools for model evaluation and diagnostics\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Mathematical functions and utilities\n",
    "from math import sqrt\n",
    "\n",
    "# Neural network and machine learning libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Model selection and hyperparameter tuning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pmdarima as pm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from scipy.stats import randint\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# to save our selected model\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_row', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Remote Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''environment_variables = dotenv_values('.env')\n",
    "\n",
    "# getting the values of the variables stored in the .env file\n",
    "database = environment_variables.get('DATABASE')\n",
    "server   = environment_variables.get('SERVER')\n",
    "username = environment_variables.get('USERNAME')\n",
    "password = environment_variables.get('PASSWORD')\n",
    "\n",
    "# below we are creating a connection string \n",
    "#connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# below we are using the connection string to connect to the server\n",
    "#connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Now the sql query to get the data is what what you see below. \n",
    "# Note that you will not have permissions to insert delete or update this database table. \n",
    "\n",
    "query1 = \"Select * from dbo.oil\"\n",
    "query2 = \"Select * from dbo.stores\"\n",
    "query3 = \"Select * from dbo.holidays_events\"\n",
    "\n",
    "# Read the sql data into dataframes\n",
    "oil = pd.read_sql(query1, connection)\n",
    "stores = pd.read_sql(query2, connection)\n",
    "holidays_events = pd.read_sql(query3, connection)\n",
    "\n",
    "oil.to_csv(\"oil.csv\", index=False)\n",
    "stores.to_csv(\"stores.csv\", index=False)\n",
    "holidays_events.to_csv(\"holidays_events.csv\", index=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining the datatypes in the columns to be sure \n",
    "\n",
    "dtypes = {'id':'int64', 'store_nbr':'int8', 'onpromotion':str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our datasets\n",
    "data_oil = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/lp3_submission_regress/lp_3_regress_sub/oil.csv\", parse_dates=['date'])\n",
    "df_stores = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/lp3_submission_regress/lp_3_regress_sub/stores.csv\")\n",
    "df_holidays_events = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/lp3_submission_regress/lp_3_regress_sub/holidays_events.csv\", dtype={'transferred': str}, parse_dates=['date'])\n",
    "df_test = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/lp3_submission_regress/lp_3_regress_sub/test.csv\", dtype=dtypes, parse_dates=['date'])\n",
    "df_transactions = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/lp3_submission_regress/lp_3_regress_sub/transactions.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"G:/AZUBI-AFRICA/CAREER_ACCELERATOR_ALL_OUT/LP3_REGR/Stockholm_lp3_colla_project_sprint3/project_folder/train.csv\", dtype=dtypes, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum dates\n",
    "min_date = df_train['date'].min()\n",
    "max_date = df_train['date'].max()\n",
    "\n",
    "print(f\"The date range is from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_time = df_train.groupby(\"date\").sales.sum().reset_index()\n",
    "sales_time = sales_time.set_index(\"date\")\n",
    "\n",
    "sales_time.plot(legend=False, figsize=(15, 7))\n",
    "plt.title(\"SALES OVER TIME\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plot indicates a rising trend in sales as the years progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train.copy()\n",
    "# Set 'date' as the index\n",
    "df_tr.set_index('date', inplace=True)\n",
    "\n",
    "# Resample and calculate the mean\n",
    "daily_sales = df_tr['sales'].resample(\"D\").mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "daily_sales.plot(title='Average Daily Sales')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and calculate the mean\n",
    "daily_sales = df_tr['sales'].resample('W').mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "daily_sales.plot(title='Average Weekly Sales')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and calculate the mean\n",
    "daily_sales = df_tr['sales'].resample('M').mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "daily_sales.plot(title='Average Monthly Sales')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stores Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute counts of each store type\n",
    "stores = df_stores['type'].value_counts()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "sns.barplot(x=stores.values, y=stores.index)\n",
    "plt.title('Counts of Store Types')\n",
    "plt.ylabel('Store Types')\n",
    "plt.xlabel(\"Number of Stores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store type D has the highest number of stores, while type E has the lowest number of stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(data=df_stores, x='city')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Stores by City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quito has the highest number of stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Stores by State\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Order the states based on their counts (from highest to lowest)\n",
    "order = df_stores['state'].value_counts().index\n",
    "\n",
    "sns.countplot(data=df_stores, x='state', order=order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Stores by State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pichincha has the highest number of stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of clusters\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Order the clusters based on their counts (from highest to lowest)\n",
    "order = df_stores['cluster'].value_counts().index\n",
    "\n",
    "sns.countplot(data=df_stores, x='cluster', order=order)\n",
    "plt.title('Distribution of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3 has the highest number of stores in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oil.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oil.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(data_oil)\n",
    "null_rows = data_oil['dcoilwtico'].isnull().sum()\n",
    "\n",
    "percentage_null = (null_rows / total_rows) * 100\n",
    "print(f\"Percentage of null values in 'dcoilwtico' column: {percentage_null:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear interpolation to impute missing values\n",
    "data_oil['dcoilwtico'] = data_oil['dcoilwtico'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum dates\n",
    "min_date = data_oil['date'].min()\n",
    "max_date = data_oil['date'].max()\n",
    "\n",
    "print(f\"The date range is from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a line plot using plotly.express\n",
    "fig = px.line(data_oil, x='date', y='dcoilwtico', title='Oil Prices Over Time')\n",
    "fig.update_xaxes(title_text='Date')\n",
    "fig.update_yaxes(title_text='Oil Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a downward trend in the prices of oil prices on the market over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=data_oil, x='dcoilwtico', bins=20, kde=True)\n",
    "plt.title(\"Distribution of Oil Prices\")\n",
    "plt.xlabel(\"Oil Price (dcoilwtico)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that the price of crude oil was heavily distributed between 40 and $60 a barrel, per day. Most of the prices of crude oil per day were between this price range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.drop(columns=['description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum dates\n",
    "min_date = df_holidays_events['date'].min()\n",
    "max_date = df_holidays_events['date'].max()\n",
    "\n",
    "print(f\"The date range is from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing holiday count by different types\n",
    "categorical_columns = ['type', 'locale', 'locale_name', 'transferred']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.countplot(data=df_holidays_events, x=column)\n",
    "    plt.title(f'Count of Holidays by {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuador had the highest number of holidays\n",
    "### National holidays had the highest count according to locale\n",
    "### Very few holidays were transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df_holidays_events.groupby(df_holidays_events['date'].dt.year)['date'].count().plot(kind='line')\n",
    "plt.title('Distribution of Holidays Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 2012 to 2014, we observed a consistent upward trend in the number of holidays. However, there was a noticeable dip in 2015. Interestingly, the number of holidays surged in 2016 but then followed by a decline in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred_counts = df_holidays_events[df_holidays_events['type'] == 'Transfer'].groupby(df_holidays_events['date'].dt.year)['type'].count()\n",
    "\n",
    "# Plot the count of transferred holidays by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "transferred_counts.plot(kind='bar')\n",
    "plt.title('Count of Transferred Holidays by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017 had the most transferred holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of event holidays on yearly basis\n",
    "\n",
    "event_data = df_holidays_events[df_holidays_events['type'] == 'Event']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_data.groupby(event_data['date'].dt.year).size().plot(kind='bar')\n",
    "plt.title('Distribution of Event Holidays on Yearly Basis')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 had the most holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum dates\n",
    "min_date = df_transactions['date'].min()\n",
    "max_date = df_transactions['date'].max()\n",
    "\n",
    "print(f\"The date range is from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_date = df_transactions.groupby('date').transactions.sum().reset_index()\n",
    "transactions_date = transactions_date.set_index(\"date\")\n",
    "\n",
    "transactions_date.plot(legend=False, color=\"skyblue\", figsize=(15, 7))\n",
    "plt.title('Number of Transactions Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notably, transaction volumes surge towards the end of each year and then exhibit a decline at the onset of the subsequent year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the transaction data to month\n",
    "trans_res= df_transactions.set_index('date').drop('store_nbr', axis=1).resample('M').mean()\n",
    "plt.figure(figsize=(15,6))\n",
    "trans_res.plot()\n",
    "plt.title('Average Monthly Transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by store and sum the transactions\n",
    "store_transactions = df_transactions.groupby('store_nbr')['transactions'].sum().reset_index()\n",
    "\n",
    "# Sort the transactions in descending order\n",
    "sorted_stores = store_transactions.sort_values(by='transactions', ascending=False)\n",
    "\n",
    "# Take top 10 stores for visualization\n",
    "top_10_stores = sorted_stores.head(10)\n",
    "\n",
    "# Visual representation of top 10 stores\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x='store_nbr', y='transactions', data=top_10_stores, order=top_10_stores['store_nbr'])\n",
    "plt.title(\"Top 10 Stores by Transactions\")\n",
    "plt.xlabel(\"Store Number\")\n",
    "plt.ylabel(\"Total Transactions\")\n",
    "plt.xticks(rotation=45)  # Rotate x labels for better visibility\n",
    "plt.tight_layout()  # Adjust layout for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Number 44 had the highest number of transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['family'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research / Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "Did the earthquake impact sales?\n",
    "\n",
    "Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "Did the oil price affected Sales\n",
    "\n",
    "How did holiday events affect Sales \n",
    "\n",
    "What analysis can we get from the date and its extractable features?\n",
    "\n",
    "What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum and maximum dates in the DataFrame\n",
    "min_date = df_train['date'].min()\n",
    "max_date = df_train['date'].max()\n",
    "\n",
    "# Create a date range for the entire period\n",
    "all_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Check if there are any dates missing in the DataFrame\n",
    "missing_dates = all_dates.difference(df_train['date'])\n",
    "\n",
    "if missing_dates.empty:\n",
    "    print(\"The train dataset is complete and has all the required dates.\")\n",
    "else:\n",
    "    print(f\"The train dataset is missing the following dates:\\n{missing_dates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from date and add it as a new column\n",
    "df_train['year'] = df_train['date'].dt.year\n",
    "\n",
    "# Group by year and find the date with max and min sales\n",
    "max_sales_dates = df_train.loc[df_train.groupby('year')['sales'].idxmax()][['year', 'date', 'sales']]\n",
    "min_sales_dates = df_train.loc[df_train.groupby('year')['sales'].idxmin()][['year', 'date', 'sales']]\n",
    "\n",
    "# Adding a column to distinguish between max and min sales\n",
    "max_sales_dates['Type'] = 'Max Sales'\n",
    "min_sales_dates['Type'] = 'Min Sales'\n",
    "\n",
    "# Concatenating both dataframes and sorting by sales in descending order \n",
    "# (and then by year and Type for a hierarchical sort)\n",
    "result_df = pd.concat([max_sales_dates, min_sales_dates], ignore_index=True)\n",
    "result_df = result_df.sort_values(by=['sales', 'year', 'Type'], ascending=[False, True, False])\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for _, row in max_sales_dates.iterrows():\n",
    "    plt.plot(row['date'], row['sales'], 'go', label=f\"Max {row['year']}\")\n",
    "\n",
    "for _, row in min_sales_dates.iterrows():\n",
    "    plt.plot(row['date'], row['sales'], 'ro', label=f\"Min {row['year']}\")\n",
    "\n",
    "plt.plot(df_train['date'], df_train['sales'], label='Sales', alpha=0.5)\n",
    "plt.title(\"Highest and Lowest Sales Dates for Each Year\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the year 2016\n",
    "df_2016 = df_train[df_train['date'].dt.year == 2016].set_index('date')\n",
    "\n",
    "# Resample data to weekly frequency\n",
    "weekly_sales = df_2016.resample('W').sum()\n",
    "\n",
    "# Date of the earthquake\n",
    "earthquake_date = \"2016-04-16\"\n",
    "\n",
    "# Visualization for weekly sales\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=weekly_sales, x=weekly_sales.index, y='sales', label='Weekly Sales', color='blue')\n",
    "plt.axvline(pd.to_datetime(earthquake_date), color='green', linestyle='--', label='Earthquake Date')\n",
    "plt.title(\"Weekly Sales in 2016\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to monthly frequency\n",
    "monthly_sales = df_2016.resample('M').sum()\n",
    "\n",
    "# Visualization for monthly sales\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=monthly_sales, x=monthly_sales.index, y='sales', label='Monthly Sales', color='blue')\n",
    "plt.axvline(pd.to_datetime(earthquake_date), color='green', linestyle='--', label='Earthquake Date')\n",
    "plt.title(\"Monthly Sales in 2016\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_date = \"2016-04-16\"\n",
    "\n",
    "# Splitting data into pre and post earthquake\n",
    "pre_earthquake = df_2016[df_2016.index < pd.to_datetime(earthquake_date)]\n",
    "post_earthquake = df_2016[df_2016.index >= pd.to_datetime(earthquake_date)]\n",
    "\n",
    "# Extract sales data for both periods\n",
    "pre_sales = pre_earthquake['sales']\n",
    "post_sales = post_earthquake['sales']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(pre_sales, post_sales, equal_var=True)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Check if p-value is less than the common significance level of 0.05\n",
    "if p_value < 0.05:\n",
    "    print(\"The earthquake had a statistically significant impact on sales.\")\n",
    "else:\n",
    "    print(\"The earthquake did not have a statistically significant impact on sales.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The earthquake had a significant impact on the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data\n",
    "df_merged = df_stores.merge(df_train, on='store_nbr')\n",
    "\n",
    "# Visualizing sales by each group\n",
    "for group in ['cluster', 'type']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    aggregated_data = df_merged.groupby(group)['sales'].sum().sort_values()\n",
    "    sns.barplot(x=aggregated_data.index, y=aggregated_data.values, palette=\"viridis\")\n",
    "    \n",
    "    plt.title(f\"Total Sales by {group}\")\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.xlabel(group)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Visualizing sales by city and state with horizontal bar plots\n",
    "for group in ['city', 'state']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    aggregated_data = df_merged.groupby(group)['sales'].sum().sort_values()\n",
    "    sns.barplot(y=aggregated_data.index, x=aggregated_data.values, palette=\"viridis\", orient='h')\n",
    "    \n",
    "    plt.title(f\"Total Sales by {group}\")\n",
    "    plt.xlabel('Total Sales')\n",
    "    plt.ylabel(group.capitalize())\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 14 had the highest sales\n",
    "### Store types D & A had the highest sales\n",
    "### Stores in Quito had the highest sales\n",
    "### Stores in Pichincha had the highest sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales and oil prices on 'date'\n",
    "df_merged = pd.merge(df_train, data_oil, on='date', how='left')\n",
    "\n",
    "# Merge the result with holidays on 'date'\n",
    "df_merged = pd.merge(df_merged, df_holidays_events, on='date', how='left')\n",
    "\n",
    "# Fill NaN values in the 'type', 'locale', 'locale_name', 'description', 'transferred' columns with 'NoHoliday' or appropriate placeholder\n",
    "df_merged[['type', 'locale', 'locale_name']] = df_merged[['type', 'locale', 'locale_name']].fillna('NoHoliday')\n",
    "df_merged['transferred'] = df_merged['transferred'].fillna(False)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Did the oil price affected Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by oil price and then calculating the total sales\n",
    "oil_price_sales = df_merged.groupby('dcoilwtico')['sales'].sum()\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=oil_price_sales.index, y=oil_price_sales.values)\n",
    "plt.title('Total Sales vs Oil Price')\n",
    "plt.xlabel('Oil Price (dcoilwtico)')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher Sales at Lower Oil Prices: There's a dense concentration of sales data points when the oil price is on the lower end. This indicates that when oil prices were lower, sales were generally higher.\n",
    "\n",
    "Sales Dwindle with Rising Oil Prices: As oil prices increase, there's a noticeable reduction in the concentration of sales. This suggests that higher oil prices might have a dampening effect on sales.\n",
    "\n",
    "In summary, there's a trend suggesting that the oil price significantly impacts sales. Lower oil prices are associated with higher sales, whereas higher oil prices correspond with reduced sales concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How did holiday events affect Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold sales data for each holiday type\n",
    "data_groups = [df_merged['sales'][df_merged['type'] == holiday_type] for holiday_type in df_merged['type'].unique()]\n",
    "\n",
    "# Perform ANOVA\n",
    "f_value, p_value = stats.f_oneway(*data_groups)\n",
    "\n",
    "print(f\"F-value: {f_value}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The different holiday types have a statistically significant impact on sales.\")\n",
    "else:\n",
    "    print(\"The different holiday types do not have a statistically significant impact on sales.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by holiday_type and then calculating average sales\n",
    "holiday_sales = df_merged.groupby('type')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=holiday_sales.index, y=holiday_sales.values, palette=\"viridis\")\n",
    "plt.title('Average Sales during Holiday Types')\n",
    "plt.xlabel('Holiday Type')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "\n",
    "# extract year, quarter, month, day, and weekday information from the date column\n",
    "df_train_copy['year'] = df_train_copy['date'].dt.year\n",
    "df_train_copy['month'] = df_train_copy['date'].dt.month\n",
    "df_train_copy['week'] = df_train_copy['date'].dt.isocalendar().week\n",
    "df_train_copy['day'] = df_train_copy['date'].dt.day\n",
    "df_train_copy['weekday'] = df_train_copy['date'].dt.weekday\n",
    "\n",
    "# plot the aggregated sales data by year\n",
    "grouped_by_year = df_train_copy.groupby('year')['sales'].sum()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(grouped_by_year.index, grouped_by_year.values)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Year\")\n",
    "plt.show()\n",
    "\n",
    "# plot the aggregated sales data by month\n",
    "grouped_by_month = df_train_copy.groupby('month')['sales'].sum()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(grouped_by_month.index, grouped_by_month.values)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Month\")\n",
    "plt.show()\n",
    "\n",
    "# plot the aggregated sales data by week\n",
    "grouped_by_week = df_train_copy.groupby('week')['sales'].sum()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(grouped_by_week.index, grouped_by_week.values)\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Week\")\n",
    "plt.show()\n",
    "\n",
    "# plot the aggregated sales data by day\n",
    "grouped_by_day = df_train_copy.groupby('day')['sales'].sum()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(grouped_by_day.index, grouped_by_day.values)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Day\")\n",
    "plt.show()\n",
    "\n",
    "# plot the aggregated sales data by weekday\n",
    "grouped_by_weekday = df_train_copy.groupby('weekday')['sales'].sum()\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(grouped_by_weekday.index, grouped_by_weekday.values)\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Weekday\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Decomposition of Sales Data\n",
    "\n",
    "The given code aims to decompose the sales data over various time frames and granularities. By breaking down the data based on year, month, week, day, and weekday, it provides a comprehensive view of sales patterns and trends. Here's a breakdown of the code:\n",
    "\n",
    "- **Year**: The year of the transaction.\n",
    "- **Month**: The month of the transaction.\n",
    "- **Week**: The week of the year the transaction occurred.\n",
    "- **Day**: The day of the month.\n",
    "- **Weekday**: The day of the week (0 denotes Monday and 6 represents Sunday).\n",
    "\n",
    "### Visualizing Sales Trends\n",
    "\n",
    "1. **Sales by Year**: A line plot showing yearly aggregated sales provides insights into the yearly sales trend and highlights any consistent growth or decline.\n",
    "\n",
    "2. **Sales by Month**: A bar plot showcasing monthly aggregated sales gives an understanding of the monthly sales distribution, potentially uncovering seasonality patterns or specific months with higher sales.\n",
    "\n",
    "3. **Sales by Week**: The line plot of weekly aggregated sales lets us see if there are specific weeks in a year where sales peak or dip.\n",
    "\n",
    "4. **Sales by Day**: By plotting daily aggregated sales, we can determine if certain days of the month consistently perform better in terms of sales.\n",
    "\n",
    "5. **Sales by Weekday**: This line plot offers insights into sales performance by weekdays, which can be helpful in understanding weekly patterns, like whether sales are higher on weekends or weekdays.\n",
    "\n",
    "In essence, this comprehensive analysis allows businesses to understand their sales patterns better, pinpointing periods of peak sales and understanding potential cyclic behaviors in their sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Mean Absolute Error (MAE)**: This metric calculates the average of absolute differences between the forecasted values and the actual values. It provides a linear penalty for each unit of difference. \n",
    "\n",
    "    \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: This metric calculates the average of the squares of the differences between the forecasted values and the actual values. It provides a quadratic penalty for each unit of difference, which means larger errors are more heavily penalized.\n",
    "\n",
    "    \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**: This is simply the square root of MSE. It's a popular metric because it represents the sample standard deviation of the differences between predicted values and observed values.\n",
    "\n",
    "    \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} \\]\n",
    "\n",
    "4. **Root Mean Squared Logarithmic Error (RMSLE)**: It calculates the difference between the log-transformed predicted and actual values. It's particularly useful when you care more about percentage errors rather than the absolute value of errors.\n",
    "\n",
    "    \\[ \\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\log(y_i + 1) - \\log(\\hat{y}_i + 1))^2} \\]\n",
    "\n",
    "**Why might MAE be greater than RMSE?**\n",
    "\n",
    "It's not necessarily true that MAE is always greater than RMSE, but it can be, depending on the dataset. Since RMSE squares the errors before averaging and then takes the square root, it tends to give more weight to larger errors. This means RMSE should be more useful when large errors are particularly undesirable. Here are some differences:\n",
    "\n",
    "- **Outliers**: RMSE is highly sensitive to outliers, whereas MAE is robust to outliers. If you have many outliers, the squared differences can lead to a much higher RMSE compared to MAE.\n",
    "  \n",
    "- **Error Distribution**: If all errors are consistently of the same magnitude, then MAE and RMSE will be similar. However, if some errors are much larger than others (a mix of small and large errors), RMSE will tend to be larger than MAE.\n",
    "\n",
    "- **Interpretability**: MAE directly represents the average error magnitude, which can sometimes be more interpretable than RMSE.\n",
    "\n",
    "In general, if you're comparing two models, it's a good practice to consider both MAE and RMSE as they can provide different insights into the errors the models are making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPOTHESIS\n",
    "\n",
    "### Null Hypothesis: Promotions do not have a positive impact on overall sales.\n",
    "\n",
    "### Alternate Hypothesis: Promotions have a positive impact on overall sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 'sales' column for each group\n",
    "sales_promotion = df_merged[df_merged['onpromotion'] == 1]['sales']\n",
    "sales_no_promotion = df_merged[df_merged['onpromotion'] == 0]['sales']\n",
    "\n",
    "# Performing a two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(sales_promotion, sales_no_promotion, equal_var=False)\n",
    "\n",
    "# Defining the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Comparing the p-value to the significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"Promotions have a significant impact on sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"There is no significant impact of promotions on sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the hypothesis test, it appears that there is no significant impact of promotions on sales. This means that there is insufficient evidence to conclude that promotions lead to a statistically significant increase in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_stores and df_train on 'store_nbr'\n",
    "df_merged = df_train.merge(df_stores, on='store_nbr', how='left')\n",
    "\n",
    "# Merge with data_oil on 'date'\n",
    "df_merged = df_merged.merge(data_oil, on='date', how='left')\n",
    "\n",
    "# Merge the result with df_holidays_events on 'date'\n",
    "df_merged = df_merged.merge(df_holidays_events, on='date', how='left')\n",
    "\n",
    "# Merge with df_transactions on 'date' and 'store_nbr'\n",
    "#df_merged = df_merged.merge(df_transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Fill NaN values in the specified columns\n",
    "df_merged[['type_y', 'locale', 'locale_name']] = df_merged[['type_y', 'locale', 'locale_name']].fillna('NoHoliday')\n",
    "df_merged['transferred'] = df_merged['transferred'].fillna(False)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating the oil prices\n",
    "df_merged['dcoilwtico'].fillna(method='ffill', inplace=True)  # Forward fill\n",
    "df_merged['dcoilwtico'].fillna(method='bfill', inplace=True)  # Backfill any remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['onpromotion'] = pd.to_numeric(df_merged['onpromotion'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df_merged.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numeric_df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'id' column as the index\n",
    "df_merged.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = [ 'city', 'state', 'type_x', 'dcoilwtico', \n",
    "                   'type_y', 'locale', 'locale_name', 'transferred']\n",
    "# Drop the columns\n",
    "df_merged = df_merged.drop(columns=columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns to make 'sales' the last column\n",
    "cols = [col for col in df_merged.columns if col != 'sales'] + ['sales']\n",
    "df_merged = df_merged[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data #'family'\n",
    "\n",
    "df_family_aggregated = df_merged.groupby(['date', 'family']).agg({\n",
    "    'sales': 'sum',\n",
    "    'onpromotion': 'sum'  # This can be a mean (indicating the proportion of items on promotion) or sum.    \n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why are we aggregating you ask?\n",
    "\n",
    "When testing properties such as presence of a unit root, what matters is the time span, not the frequency at which the data is sampled. Unless the sample is too small for asymptotic inference, that is.\n",
    "\n",
    "http://davegiles.blogspot.no/2014/05/unit-root-testing-sample-size-vs-sample.html\n",
    "http://davegiles.blogspot.no/2014/09/the-econometrics-of-temporal.html#more\n",
    "https://johnhcochrane.blogspot.com/2015/04/unit-roots-redux.html#more\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_family_aggregated.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_family_aggregated.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> Feature Engineering</center></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales can vary significantly depending on the day of the week. For example, retailers might observe more sales on weekends as compared to weekdays. By including the day of the week as a predictor, the model can capture such weekly patterns.\n",
    "\n",
    "Lagged variables help the model understand temporal dependencies. If sales today depend on sales from the previous day or the same day from the previous week, these lagged variables can capture that relationship.\n",
    "\n",
    "Rolling statistics can smooth out short-term fluctuations and highlight longer-term trends or cycles in the data. This helps in capturing seasonality or patterns that recur over fixed periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the prediction of sales, the dataset has been enhanced by:\n",
    "\n",
    "Incorporating the day of the week to account for weekly sales patterns.\n",
    "\n",
    "Adding lagged sales variables to capture short-term dependencies and potential weekly patterns.\n",
    "\n",
    "Introducing rolling statistics, specifically a 7-day rolling mean, to smooth out short-term variations and emphasize longer-term trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Day of the week (0 = Monday, 6 = Sunday)\n",
    "df_family_aggregated['day_of_week'] = df_family_aggregated['date'].dt.dayofweek\n",
    "\n",
    "# 2. Lagged variables\n",
    "# Here I'm creating a 1-day lag for sales. \n",
    "df_family_aggregated['lag_1'] = df_family_aggregated['sales'].shift(1)\n",
    "\n",
    "# df_family_aggregated['lag_7'] = df_family_aggregated['sales'].shift(7)  # 1-week lag\n",
    "\n",
    "# 3. Rolling statistics\n",
    "window_size = 7  # 7-day window, you can adjust this based on your data's frequency and need\n",
    "df_family_aggregated['rolling_mean'] = df_family_aggregated['sales'].rolling(window=window_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_family_aggregated.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with NaN values that were introduced due to lag and rolling stats calculations\n",
    "df_family_aggregated.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_family_aggregated.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the new Variables added are statistically significant in predicting Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our independent variables (X) and dependent variable (y)\n",
    "X = df_family_aggregated[['onpromotion', 'day_of_week', 'lag_1', 'rolling_mean']]\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "y = df_family_aggregated['sales']\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the regression results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "const: The p-value is practically 0 (significantly less than 0.05). This means that the intercept (const) is statistically significant, and its estimated value is different from zero in the population.\n",
    "\n",
    "onpromotion: The p-value is also essentially 0. This indicates that the coefficient for the onpromotion variable is statistically significant, implying that promotions have a significant effect on sales.\n",
    "\n",
    "day_of_week: The p-value here is less than 0.05, suggesting that the day_of_week coefficient is statistically significant. Therefore, the day of the week has a significant influence on sales.\n",
    "\n",
    "lag_1: With a p-value that's virtually 0, the lag_1 coefficient is statistically significant. This implies that the sales value of the previous day (or period) significantly affects the current sales.\n",
    "\n",
    "rolling_mean: The p-value for this variable is again essentially 0. It suggests that the rolling mean is a statistically significant predictor for sales in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the VIF test to check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIF scores for multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF = 1: No multicollinearity.\n",
    "\n",
    "1 < VIF < 5: Moderately correlated. In many fields, a VIF below 5 is considered acceptable.\n",
    "\n",
    "VIF >= 5: Highly correlated. A VIF above 5 is typically considered to indicate high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "const: VIF of 4.142 suggests that the intercept is moderately correlated with other predictors. This is not typically a concern since it's the constant.\n",
    "\n",
    "onpromotion: A VIF of 1.1527 indicates little to no multicollinearity.\n",
    "\n",
    "day_of_week: With a VIF close to 1 (1.0181), this predictor is not showing multicollinearity.\n",
    "\n",
    "lag_1: A VIF of 1.1991 also suggests very minimal multicollinearity.\n",
    "\n",
    "rolling_mean: A VIF of 1.3610 indicates little multicollinearity but slightly more than the other predictors (excluding the constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> Visualizing the Sales Column</center></h1> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_family_aggregated['sales'].plot(title='Total Daily Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autocorrelation Function (ACF) is a tool that provides information about the linear relationship between values of a time series separated by various time lags. In simpler terms, it tells you how correlated a time series is with itself at different lagged values.\n",
    "A stationary time series will have an ACF that drops to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for a non-stationary series, the ACF will often have values that remain significant over several lags, which is indicative of a strong time-dependent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF Plot\n",
    "plot_acf(df_family_aggregated['sales'], lags=40)\n",
    "plt.title('Autocorrelation Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stationary time series will have an ACF that decays to zero relatively quickly. This means that the correlation between the current value of the time series and its past values decreases as the lag increases. The ACF decays to zero relatively quickly meaning it is stationary but we have to do a formal test to confirm this \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> Augmented Dickey-Fuller (ADF)</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Augmented Dickey-Fuller (ADF) test is a formal statistical test for stationarity. Specifically, it's used to determine the presence of a unit root in a time series, which would suggest that the time series is non-stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "result = adfuller(df_family_aggregated['sales'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:', result[4])\n",
    "if result[1] <= 0.05:\n",
    "    print(\"The time series is stationary.\")\n",
    "else:\n",
    "    print(\"The time series is not stationary.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> DATA PREPROCESSING</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for family column\n",
    "df_family_dummies = pd.get_dummies(df_family_aggregated['family'], drop_first=True, prefix='family')\n",
    "df_family_aggregated = pd.concat([df_family_aggregated, df_family_dummies], axis=1)\n",
    "df_family_aggregated.drop('family', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to scale\n",
    "columns_to_scale = ['onpromotion', 'day_of_week', 'lag_1', 'rolling_mean', 'sales'] + list(df_family_dummies.columns)#'is_holiday'\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_family_aggregated[columns_to_scale] = scaler.fit_transform(df_family_aggregated[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_family_aggregated = df_family_aggregated.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df_family_aggregated))\n",
    "train_data = df_family_aggregated[:train_size]\n",
    "test_data = df_family_aggregated[train_size:]\n",
    "\n",
    "X_train = train_data.drop(columns=['sales', 'date'])\n",
    "y_train = train_data['sales']\n",
    "X_test = test_data.drop(columns=['sales', 'date'])\n",
    "y_test = test_data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>rolling_mean</th>\n",
       "      <th>family_BABY CARE</th>\n",
       "      <th>family_BEAUTY</th>\n",
       "      <th>family_BEVERAGES</th>\n",
       "      <th>family_BOOKS</th>\n",
       "      <th>family_BREAD/BAKERY</th>\n",
       "      <th>family_CELEBRATION</th>\n",
       "      <th>...</th>\n",
       "      <th>family_MAGAZINES</th>\n",
       "      <th>family_MEATS</th>\n",
       "      <th>family_PERSONAL CARE</th>\n",
       "      <th>family_PET SUPPLIES</th>\n",
       "      <th>family_PLAYERS AND ELECTRONICS</th>\n",
       "      <th>family_POULTRY</th>\n",
       "      <th>family_PREPARED FOODS</th>\n",
       "      <th>family_PRODUCE</th>\n",
       "      <th>family_SCHOOL AND OFFICE SUPPLIES</th>\n",
       "      <th>family_SEAFOOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    onpromotion  day_of_week     lag_1  rolling_mean  family_BABY CARE  \\\n",
       "6           0.0     0.166667  0.000190      0.000642               0.0   \n",
       "7           0.0     0.166667  0.000000      0.000763               0.0   \n",
       "8           0.0     0.166667  0.000195      0.000856               0.0   \n",
       "9           0.0     0.166667  0.000150      0.000900               0.0   \n",
       "10          0.0     0.166667  0.000075      0.000405               0.0   \n",
       "\n",
       "    family_BEAUTY  family_BEVERAGES  family_BOOKS  family_BREAD/BAKERY  \\\n",
       "6             0.0               0.0           0.0                  0.0   \n",
       "7             0.0               0.0           0.0                  0.0   \n",
       "8             0.0               0.0           0.0                  0.0   \n",
       "9             0.0               0.0           0.0                  0.0   \n",
       "10            0.0               0.0           0.0                  0.0   \n",
       "\n",
       "    family_CELEBRATION  ...  family_MAGAZINES  family_MEATS  \\\n",
       "6                  1.0  ...               0.0           0.0   \n",
       "7                  0.0  ...               0.0           0.0   \n",
       "8                  0.0  ...               0.0           0.0   \n",
       "9                  0.0  ...               0.0           0.0   \n",
       "10                 0.0  ...               0.0           0.0   \n",
       "\n",
       "    family_PERSONAL CARE  family_PET SUPPLIES  family_PLAYERS AND ELECTRONICS  \\\n",
       "6                    0.0                  0.0                             0.0   \n",
       "7                    0.0                  0.0                             0.0   \n",
       "8                    0.0                  0.0                             0.0   \n",
       "9                    0.0                  0.0                             0.0   \n",
       "10                   0.0                  0.0                             0.0   \n",
       "\n",
       "    family_POULTRY  family_PREPARED FOODS  family_PRODUCE  \\\n",
       "6              0.0                    0.0             0.0   \n",
       "7              0.0                    0.0             0.0   \n",
       "8              0.0                    0.0             0.0   \n",
       "9              0.0                    0.0             0.0   \n",
       "10             0.0                    0.0             0.0   \n",
       "\n",
       "    family_SCHOOL AND OFFICE SUPPLIES  family_SEAFOOD  \n",
       "6                                 0.0             0.0  \n",
       "7                                 0.0             0.0  \n",
       "8                                 0.0             0.0  \n",
       "9                                 0.0             0.0  \n",
       "10                                0.0             0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are saving the train set as a csv file so we can load when creating our streamlit app\n",
    "\n",
    "X_train.to_csv('X_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> Model Selection</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA: Effective for data with no seasonality. Works well when a series is stationary or can be made stationary through differencing (hence the \"Integrated\" part in ARIMA).\n",
    "\n",
    "SARIMA: An extension of ARIMA to handle seasonality in the data.\n",
    "\n",
    "Exponential Smoothing: Useful for data with a trend or seasonality. There are variations like Holt (for trend) and Holt-Winters (for both trend and seasonality).\n",
    "\n",
    "LSTM: A type of recurrent neural network. Can capture long-term dependencies and complex patterns in the data. Typically requires more data and more computational resource\n",
    "\n",
    "XGB Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> ACF AND PACF</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) plots are fundamental diagnostic tools. They help us understand the underlying characteristics of a time series dataset, and more specifically, they are typically used when modeling the data using Autoregressive Integrated Moving Average (ARIMA) models or its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(df_family_aggregated['sales'], lags=80)  # you can adjust the lags as per your data's requirements\n",
    "plt.title('ACF Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PACF\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_pacf(df_family_aggregated['sales'], lags=40)  # again, adjust lags as necessary\n",
    "plt.title('PACF Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appropriate ARIMA model for our dataset is ARIMA(1,0,0). This is because only the first lag exhibits statistical significance, suggesting an autoregressive order of 1. Additionally, there's no indication of a moving average component, and differencing of the target variable was not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> FITTING MODELS</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the exogenous variables\n",
    "\n",
    "exog_features = ['onpromotion', 'day_of_week', 'lag_1', 'rolling_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSLE Definition\n",
    "def rmsle(predicted, actual):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(predicted) - np.log1p(actual))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the ValueWarning warnings\n",
    "warnings.simplefilter(action='ignore', category=ValueWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Arima Model\n",
    "model_arima = sm.tsa.SARIMAX(y_train, exog=X_train[exog_features], order=(1,0,0))\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "forecast_arima = results_arima.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1, exog=X_test[exog_features])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_arima, color='red', label='ARIMA Forecast')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "mse_arima = mean_squared_error(y_test, forecast_arima)\n",
    "rmse_arima = np.sqrt(mse_arima)\n",
    "rmsle_arima = rmsle(forecast_arima, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting sarima Model\n",
    "model_sarima = sm.tsa.SARIMAX(y_train, exog=X_train[exog_features], order=(1,1,0), seasonal_order=(1,0,0,12))\n",
    "results_sarima = model_sarima.fit(disp=False)\n",
    "forecast_sarima = results_sarima.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1, exog=X_test[exog_features])\n",
    "\n",
    "#Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_sarima, color='red', label='SARIMA Forecast')\n",
    "plt.legend()\n",
    "plt.title('SARIMA Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "mse_sarima = mean_squared_error(y_test, forecast_sarima)\n",
    "rmse_sarima = np.sqrt(mse_sarima)\n",
    "rmsle_sarima = rmsle(forecast_sarima, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit ETS Model and Make Initial Forecasts\n",
    "model_ets = ExponentialSmoothing(y_train, seasonal='add', seasonal_periods=12)\n",
    "results_ets = model_ets.fit()\n",
    "initial_forecast = results_ets.forecast(steps=len(y_test))\n",
    "\n",
    "# 2. Prepare Adjusted Exogenous Data for Linear Regression\n",
    "\n",
    "# Using the initial forecast and exog features to predict y_test.\n",
    "X_adjust_train = pd.concat([y_train.shift(-1).dropna().rename(\"forecast\"), X_train[exog_features].iloc[1:, :]], axis=1)\n",
    "X_adjust_test = pd.concat([pd.Series(initial_forecast, index=y_test.index).rename(\"forecast\"), X_test[exog_features]], axis=1)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "X_adjust_train = X_adjust_train.dropna()\n",
    "y_adjust_train = y_train.loc[X_adjust_train.index]\n",
    "\n",
    "X_adjust_test = X_adjust_test.dropna()\n",
    "y_adjust_test = y_test.loc[X_adjust_test.index]\n",
    "\n",
    "# Convert all feature names to strings\n",
    "X_adjust_train.columns = X_adjust_train.columns.astype(str)\n",
    "X_adjust_test.columns = X_adjust_test.columns.astype(str)\n",
    "\n",
    "# 3. Fit the Linear Regression Model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_adjust_train, y_adjust_train)\n",
    "forecast_ets = lr.predict(X_adjust_test)\n",
    "\n",
    "# 4. Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_adjust_test.index, y_adjust_test, label='Actual')\n",
    "plt.plot(y_adjust_test.index, forecast_ets, color='red', label='Adjusted ETS Forecast')\n",
    "plt.legend()\n",
    "plt.title('Adjusted ETS Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "# 5. Performance Metrics\n",
    "mse_ets = mean_squared_error(y_adjust_test, forecast_ets)\n",
    "rmse_ets = np.sqrt(mse_ets)\n",
    "rmsle_ets = rmsle(forecast_ets, y_adjust_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting XGB Boost Regressor\n",
    "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "model_xgb.fit(X_train, y_train)\n",
    "forecast_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_xgb, color='red', label='XGBoost Forecast')\n",
    "plt.legend()\n",
    "plt.title('XGBoost Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, forecast_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "rmsle_xgb = rmsle(forecast_xgb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = X_train[exog_features].values.reshape(X_train.shape[0], 1, len(exog_features))\n",
    "X_test_values = X_test[exog_features].values.reshape(X_test.shape[0], 1, len(exog_features))\n",
    "\n",
    "# LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, input_shape=(X_train_values.shape[1], X_train_values.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "model_lstm.fit(X_train_values, y_train, epochs=50, batch_size=72, validation_data=(X_test_values, y_test), verbose=2, shuffle=False)\n",
    "\n",
    "forecast_lstm = model_lstm.predict(X_test_values).flatten()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_lstm, color='red', label='LSTM Forecast')\n",
    "plt.legend()\n",
    "plt.title('LSTM Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test, forecast_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "rmsle_lstm = rmsle(forecast_lstm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of RMSE and MSE for each model\n",
    "data = {\n",
    "    'Model': ['ARIMA', 'SARIMA', 'ExponentialSmoothing', 'XGBoost', 'LSTM'],\n",
    "    'RMSE': [rmse_arima, rmse_sarima, rmse_ets, rmse_xgb, rmse_lstm],\n",
    "    'MSE': [mse_arima, mse_sarima, mse_ets, mse_xgb, mse_lstm],\n",
    "    'RMSLE': [rmsle_arima, rmsle_sarima, rmsle_ets, rmsle_xgb, rmsle_lstm]\n",
    "}\n",
    "\n",
    "range_of_y_test = y_test.max() - y_test.min()\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "score_df = pd.DataFrame(data)\n",
    "\n",
    "percentage_rmse = score_df['RMSE'] * 100\n",
    "percentage_mse = score_df['MSE'] * 100\n",
    "\n",
    "score_df['RMSE (%)'] = percentage_rmse\n",
    "score_df['MSE (%)'] = percentage_mse\n",
    "\n",
    "# Display the scores table\n",
    "print(score_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> ADVANCED MODEL EVALUATION</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIme series splitting for cross Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validating ARIMA Model\n",
    "mse_scores_arima = []\n",
    "rmsle_scores_arima = []  # List to collect RMSLE scores\n",
    "\n",
    "for train_index, test_index in tscv.split(y_train):\n",
    "    train, test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    exog_train, exog_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    \n",
    "    model_arima = sm.tsa.SARIMAX(train, exog=exog_train[exog_features], order=(1,0,0))\n",
    "    results_arima = model_arima.fit(disp=False)\n",
    "    forecast_arima = results_arima.predict(start=len(train), end=len(train) + len(test) - 1, exog=exog_test[exog_features])\n",
    "    \n",
    "    mse_arima = mean_squared_error(test, forecast_arima)\n",
    "    mse_scores_arima.append(mse_arima)\n",
    "\n",
    "    # Computing RMSLE using your custom function\n",
    "    rmsle_value = rmsle(forecast_arima, test)\n",
    "    rmsle_scores_arima.append(rmsle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores_sarima = []\n",
    "rmsle_scores_sarima = []  # List to collect RMSLE scores\n",
    "\n",
    "for train_index, test_index in tscv.split(y_train):\n",
    "    train, test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    exog_train, exog_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    \n",
    "    model_sarima = sm.tsa.SARIMAX(train, exog=exog_train[exog_features], order=(1,1,0), seasonal_order=(1,0,0,12))\n",
    "    results_sarima = model_sarima.fit(disp=False)\n",
    "    forecast_sarima = results_sarima.predict(start=len(train), end=len(train) + len(test) - 1, exog=exog_test[exog_features])\n",
    "    \n",
    "    mse_sarima = mean_squared_error(test, forecast_sarima)\n",
    "    mse_scores_sarima.append(mse_sarima)\n",
    "\n",
    "    # Computing RMSLE using your custom function\n",
    "    rmsle_value = rmsle(forecast_sarima, test)\n",
    "    rmsle_scores_sarima.append(rmsle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores_ets = []\n",
    "rmsle_scores_ets = []  # List to collect RMSLE scores\n",
    "\n",
    "for train_index, test_index in tscv.split(y_train):\n",
    "    train, test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    exog_train, exog_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    \n",
    "    model_ets = ExponentialSmoothing(train, seasonal='add', seasonal_periods=12)\n",
    "    results_ets = model_ets.fit()\n",
    "    initial_forecast = results_ets.forecast(steps=len(test))\n",
    "\n",
    "    X_adjust_train = pd.concat([train.shift(-1).dropna().rename(\"forecast\"), exog_train[exog_features].iloc[1:, :]], axis=1)\n",
    "    X_adjust_test = pd.concat([pd.Series(initial_forecast, index=test.index).rename(\"forecast\"), exog_test[exog_features]], axis=1)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    X_adjust_train = X_adjust_train.dropna()\n",
    "    y_adjust_train = train.loc[X_adjust_train.index]\n",
    "\n",
    "    X_adjust_test = X_adjust_test.dropna()\n",
    "    y_adjust_test = test.loc[X_adjust_test.index]\n",
    "\n",
    "    # Convert all feature names to strings\n",
    "    X_adjust_train.columns = X_adjust_train.columns.astype(str)\n",
    "    X_adjust_test.columns = X_adjust_test.columns.astype(str)\n",
    "\n",
    "    # Fit the Linear Regression Model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_adjust_train, y_adjust_train)\n",
    "    forecast_ets = lr.predict(X_adjust_test)\n",
    "    \n",
    "    mse_ets = mean_squared_error(y_adjust_test, forecast_ets)\n",
    "    mse_scores_ets.append(mse_ets)\n",
    "\n",
    "    # Computing RMSLE using your custom function\n",
    "    rmsle_value = rmsle(forecast_ets, y_adjust_test)\n",
    "    rmsle_scores_ets.append(rmsle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores_xgb = []\n",
    "rmsle_scores_xgb = []  # List to collect RMSLE scores\n",
    "\n",
    "for train_index, test_index in tscv.split(y_train):\n",
    "    train, test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    exog_train, exog_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    \n",
    "    model_xgb = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "    model_xgb.fit(exog_train, train)\n",
    "    forecast_xgb = model_xgb.predict(exog_test)\n",
    "    \n",
    "    mse_xgb = mean_squared_error(test, forecast_xgb)\n",
    "    mse_scores_xgb.append(mse_xgb)\n",
    "\n",
    "    # Computing RMSLE using your custom function\n",
    "    rmsle_value = rmsle(forecast_xgb, test)\n",
    "    rmsle_scores_xgb.append(rmsle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation of LSTM Model \n",
    "rmsle_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_train_values):\n",
    "    \n",
    "    X_train_cv, X_test_cv = X_train_values[train_index], X_train_values[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(50, input_shape=(X_train_cv.shape[1], X_train_cv.shape[2])))\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(loss='mse', optimizer='adam')\n",
    "    model_lstm.fit(X_train_cv, y_train_cv, epochs=50, batch_size=72, validation_data=(X_test_cv, y_test_cv), verbose=2, shuffle=False)\n",
    "    \n",
    "    forecast_lstm = model_lstm.predict(X_test_cv).flatten()\n",
    "    score = rmsle(forecast_lstm, y_test_cv)\n",
    "    rmsle_scores.append(score)\n",
    "    print(f'RMSLE Score for split {len(rmsle_scores)}: {score}')\n",
    "\n",
    "print(f'Average RMSLE Score: {np.mean(rmsle_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for RMSLE\n",
    "df_rmsle_scores = pd.DataFrame({\n",
    "    'ARIMA': rmsle_scores_arima,\n",
    "    'SARIMA': rmsle_scores_sarima,\n",
    "    'ETS': rmsle_scores_ets,\n",
    "    'XGBoost': rmsle_scores_xgb,\n",
    "    'LSTM' : rmsle_scores  \n",
    "})\n",
    "\n",
    "# Compute the average RMSLE across all folds for each model\n",
    "avg_rmsle = df_rmsle_scores.mean().sort_values(ascending=True)  # Lower RMSLE values are better, hence sort in ascending\n",
    "\n",
    "print(avg_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: With an RMSLE of 0.007496, this model demonstrates the best performance among all the models tested. It has the closest predictions to the actual data, with the error being the smallest.\n",
    "\n",
    "LSTM: The LSTM model, a type of recurrent neural network, yielded an RMSLE of 0.034217. While it performs reasonably well, it's not as precise as the XGBoost model.\n",
    "\n",
    "ETS: The Exponential Smoothing model has an RMSLE of 0.035308, which is close to the LSTM model's performance but slightly worse. ETS is a time series forecasting method that captures different time series components like trend and seasonality.\n",
    "\n",
    "ARIMA: The AutoRegressive Integrated Moving Average model produced an RMSLE of 0.051475. ARIMA is a classical method in time series forecasting that combines autoregressive, differencing, and moving average components.\n",
    "\n",
    "SARIMA: The Seasonal ARIMA, which extends ARIMA by adding a seasonal component, has the highest RMSLE of 0.054316. This suggests it's the least accurate among the models tested on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SARIMA model was dropped from consideration due to its high RMSLE score of 0.054316. This score indicates that, among the models tested, SARIMA had the least accurate predictions, making other models, especially XGBoost with its significantly lower RMSLE, preferable choices for this forecasting task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> HYPERPARAMETER TUNING</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ARIMA\n",
    "auto_arima_model = pm.auto_arima(y_train, exog=X_train[exog_features], trace=True, error_action='ignore', suppress_warnings=True)\n",
    "print(auto_arima_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified ETS hyperparameters to try\n",
    "ets_param_grid = {\n",
    "    'trend': ['add', None],\n",
    "    'seasonal': ['add', None],\n",
    "    'seasonal_periods': list(range(7, 365, 12))  \n",
    "}\n",
    "best_ets_params = None\n",
    "best_ets_mse = float(\"inf\")\n",
    "\n",
    "# ETS hyperparameter tuning\n",
    "for param in ParameterGrid(ets_param_grid):\n",
    "    try:\n",
    "        model_ets = ExponentialSmoothing(y_train, **param)\n",
    "        results_ets = model_ets.fit()\n",
    "        forecast_ets = results_ets.forecast(steps=len(y_test))\n",
    "        mse_ets = mean_squared_error(y_test, forecast_ets)\n",
    "        if mse_ets < best_ets_mse:\n",
    "            best_ets_mse = mse_ets\n",
    "            best_ets_params = param\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"Best ETS parameters:\", best_ets_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost hyperparameters to try\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6]\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "grid_search = GridSearchCV(model_xgb, xgb_param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best XGBoost parameters:\", best_xgb_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> FITTING TUNED MODELS</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best parameters for ARIMA\n",
    "model_arima = sm.tsa.SARIMAX(y_train, \n",
    "                            exog=X_train[exog_features], \n",
    "                            order=(5, 1, 2))  # Updated order parameters\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "forecast_arima = results_arima.predict(start=len(y_train), \n",
    "                                       end=len(y_train) + len(y_test) - 1, \n",
    "                                       exog=X_test[exog_features])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_arima, color='red', label='ARIMA Forecast')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "mse_arima = mean_squared_error(y_test, forecast_arima)\n",
    "rmse_arima = np.sqrt(mse_arima)\n",
    "rmsle_arima_best = rmsle(forecast_arima, y_test)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse_arima)\n",
    "print(\"RMSLE\", rmsle_arima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Fit ETS Model and Make Initial Forecasts using Best Parameters\n",
    "model_ets = ExponentialSmoothing(y_train, seasonal='add', seasonal_periods=55, trend='add')\n",
    "results_ets = model_ets.fit()\n",
    "initial_forecast = results_ets.forecast(steps=len(y_test))\n",
    "\n",
    "# 2. Prepare Adjusted Exogenous Data for Linear Regression\n",
    "\n",
    "# Using the initial forecast and exog features to predict y_test.\n",
    "X_adjust_train = pd.concat([y_train.shift(-1).dropna().rename(\"forecast\"), X_train[exog_features].iloc[1:, :]], axis=1)\n",
    "X_adjust_test = pd.concat([pd.Series(initial_forecast, index=y_test.index).rename(\"forecast\"), X_test[exog_features]], axis=1)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "X_adjust_train = X_adjust_train.dropna()\n",
    "y_adjust_train = y_train.loc[X_adjust_train.index]\n",
    "\n",
    "X_adjust_test = X_adjust_test.dropna()\n",
    "y_adjust_test = y_test.loc[X_adjust_test.index]\n",
    "\n",
    "# Convert all feature names to strings\n",
    "X_adjust_train.columns = X_adjust_train.columns.astype(str)\n",
    "X_adjust_test.columns = X_adjust_test.columns.astype(str)\n",
    "\n",
    "# 3. Fit the Linear Regression Model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_adjust_train, y_adjust_train)\n",
    "forecast_ets = lr.predict(X_adjust_test)\n",
    "\n",
    "# 4. Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(y_adjust_test.index, y_adjust_test, label='Actual')\n",
    "plt.plot(y_adjust_test.index, forecast_ets, color='red', label='Adjusted ETS Forecast')\n",
    "plt.legend()\n",
    "plt.title('Adjusted ETS Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "# 5. Performance Metrics\n",
    "mse_ets = mean_squared_error(y_adjust_test, forecast_ets)\n",
    "rmse_ets = np.sqrt(mse_ets)\n",
    "rmsle_ets_best = rmsle(forecast_ets, y_adjust_test)\n",
    "\n",
    "\n",
    "print(\"RMSLE\", rmsle_ets_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best parameters to fit the XGBoost model\n",
    "best_params_xgb = {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 200}\n",
    "\n",
    "model_xgb_best = xgb.XGBRegressor(**best_params_xgb, objective='reg:squarederror')\n",
    "model_xgb_best.fit(X_train, y_train)\n",
    "forecast_xgb_best = model_xgb_best.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data['date'], y_test, label='Actual')\n",
    "plt.plot(test_data['date'], forecast_xgb_best, color='red', label='XGBoost Forecast')\n",
    "plt.legend()\n",
    "plt.title('XGBoost Forecast vs Actuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "mse_xgb_best = mean_squared_error(y_test, forecast_xgb_best)\n",
    "rmse_xgb_best = np.sqrt(mse_xgb_best)\n",
    "rmsle_xgb_best = rmsle(forecast_xgb_best, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RMSlE with best XGBoost parameters:\", rmsle_xgb_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an extensive hyperparameter tuning process, we identified the optimal settings for our XGBoost model. The best model, equipped with these fine-tuned parameters, achieved an impressive Root Mean Squared Logarithmic Error (RMSLE) of 0.0053827804583248175. This low RMSLE score underscores the model's accuracy and the efficacy of the tuning process in optimizing its performance and predicting sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1 style='background:#0A4D68; border:3; color:cyan; border-color:cyan; border-style:dotted;'><center> TEST PREDICTIONS</center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data\n",
    "df_test_aggregated = df_test.groupby(['date', 'family']).agg({\n",
    "    'onpromotion': 'sum'    \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_aggregated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Day of the week\n",
    "df_test_aggregated['day_of_week'] = df_test_aggregated['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lagged variables\n",
    "# Since this is test data and we do not have prior data to create lag, \n",
    "# we can set lagged values to 0 \n",
    "df_test_aggregated['lag_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rolling statistics\n",
    "# Same as lagged variables, since this is test data, we can't compute rolling mean.\n",
    "# We can set it to 0 \n",
    "df_test_aggregated['rolling_mean'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for family column\n",
    "df_test_dummies = pd.get_dummies(df_test_aggregated['family'], drop_first=True, prefix='family')\n",
    "df_test_aggregated = pd.concat([df_test_aggregated, df_test_dummies], axis=1)\n",
    "df_test_aggregated.drop('family', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new scaler without the 'sales' column\n",
    "columns_to_scale_without_sales = ['onpromotion', 'day_of_week', 'lag_1', 'rolling_mean'] + list(df_family_dummies.columns)\n",
    "scaler_without_sales = MinMaxScaler()\n",
    "scaler_without_sales.fit(df_family_aggregated[columns_to_scale_without_sales])\n",
    "\n",
    "# Now, use this new scaler to transform the test set\n",
    "df_test_aggregated[columns_to_scale_without_sales] = scaler_without_sales.transform(df_test_aggregated[columns_to_scale_without_sales])\n",
    "\n",
    "df_test_aggregated = df_test_aggregated.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure to only consider the features the model was trained on.\n",
    "features_considered = X_train.columns.tolist()\n",
    "X_new_test = df_test_aggregated[features_considered]\n",
    "\n",
    "# Predicting with the model\n",
    "forecast_xgb_new_test = model_xgb_best.predict(X_new_test)\n",
    "\n",
    "# Combine the forecasts with the aggregated dataset\n",
    "df_test_aggregated['forecast'] = forecast_xgb_new_test\n",
    "\n",
    "# Group by the dates and take the sum of the forecasts\n",
    "datewise_forecast = df_test_aggregated.groupby('date')['forecast'].sum()\n",
    "\n",
    "# Now, plot these aggregated forecasts\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(datewise_forecast.index, datewise_forecast.values, color='red', label='XGBoost Forecast for new test data')\n",
    "plt.legend()\n",
    "plt.title('XGBoost Forecast for new test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of X_train\n",
    "print(\"X_train:\")\n",
    "print(X_train.head())\n",
    "\n",
    "# Display the head of y_train\n",
    "print(\"\\ny_train:\")\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE ARE SAVING OUR MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our best trained XGBoost model to a file\n",
    "model_filename = \"xgboost_model.pkl\"\n",
    "joblib.dump(model_xgb_best, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scaler.pkl']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving our preprocessors as files\n",
    "joblib.dump(scaler, 'standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the one-hot encoding\n",
    "encoding_file_path = 'family_encoding.pkl'\n",
    "# Serialize and save the one-hot encoded DataFrame\n",
    "with open(encoding_file_path, 'wb') as file:\n",
    "    pickle.dump(df_family_aggregated, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
